import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { db, COLLECTIONS } from "@/lib/firebase-admin";
import { generateConversationTitle } from "@/lib/gemini";
import { getActivePrompt, processPromptVariables } from "@/lib/prompts";
import { ProviderFactory } from "@/lib/providers/provider-factory";
import { AIMessage } from "@/types/ai-providers";
import { NextRequest } from "next/server";
import { loadMemoryForChat, cleanupUserMemory } from "@/lib/memory";
import { FileAttachment, Message } from "@/types";
import { keywordSystem } from "@/lib/keywords/triggers";
import { KeywordTriggerType } from "@/lib/keywords/system";
import { isIntelligentAnalysisEnabled, isWebSearchEnabled, isAgenticModeEnabled } from "@/config/feature-flags";
import { createAgent } from "@/lib/agent";
import { AgentEvent } from "@/types/agent";
import { promptAnalyzer } from "@/lib/prompt-analysis/analyzer";
import { contextOrchestrator } from "@/lib/context-engineering/orchestrator";
import { addMemoryFacts, generateMemoryId, calculateExpiry, getUserMemory, saveUserMemory } from "@/lib/memory/storage";
import { ProgressEmitter, registerEmitter, removeEmitter } from "@/lib/progress/emitter";
import { ProgressStep, ProgressEvent } from "@/lib/progress/types";
import { SearchResult } from "@/types/web-search";
import { PromptAnalysisResult } from "@/types/prompt-analysis";
import { MemoryFact } from "@/types/memory";
import { convertConversationToWhimBlocks } from "@/lib/whim/converter";
import { Timestamp } from 'firebase-admin/firestore';
import { Whim } from "@/types/whim";
import { promptEnhancer } from "@/lib/image/prompt-enhancer";

export async function POST(req: NextRequest) {
  try {
    const session = await getServerSession(authOptions);

    if (!session?.user) {
      return new Response("Unauthorized", { status: 401 });
    }

    const { conversationId, message, files, whimId, whimContext } = await req.json();

    if (!conversationId || (!message && (!files || files.length === 0))) {
      return new Response("Missing required fields", { status: 400 });
    }

    // Check if this is a whim assistant request
    const isWhimAssistant = !!(whimId && whimContext);

    // Verify conversation belongs to user
    const conversationRef = db
      .collection(COLLECTIONS.CONVERSATIONS)
      .doc(conversationId);
    const conversationDoc = await conversationRef.get();

    if (!conversationDoc.exists) {
      return new Response("Conversation not found", { status: 404 });
    }

    const conversationData = conversationDoc.data();
    if (conversationData?.user_id !== session.user.id) {
      return new Response("Forbidden", { status: 403 });
    }

    // Create progress emitter for this request
    const progressEmitter = new ProgressEmitter();
    const requestId = progressEmitter.getRequestId();
    registerEmitter(progressEmitter);

    console.log(`[Chat API] Created progress tracker: ${requestId}`);

    // Create the encoder and stream controller reference early
    const encoder = new TextEncoder();
    let streamController: ReadableStreamDefaultController<Uint8Array> | null = null;

    // Subscribe to send progress events in real-time to the stream
    const progressUnsubscribe = progressEmitter.subscribe((event) => {
      if (streamController) {
        try {
          const progressLine = `[PROGRESS]${JSON.stringify(event)}\n`;
          streamController.enqueue(encoder.encode(progressLine));
          console.log('[Server] Sent real-time progress event:', event.step, event.status);
        } catch (err) {
          console.error('[Stream] Error sending progress:', err);
        }
      }
    });

    // Save user message
    // Store file metadata without base64 data to avoid Firestore size limits
    // Fixed: Conditionally include thumbnail field to prevent undefined values
    const fileMetadata = files?.map((file: FileAttachment) => {
      const metadata: Partial<FileAttachment> = {
        id: file.id,
        name: file.name,
        type: file.type,
        mimeType: file.mimeType,
        size: file.size,
      };

      // Only include thumbnail if it exists (images have thumbnails, PDFs don't)
      if (file.thumbnail) {
        metadata.thumbnail = file.thumbnail;
      }

      return metadata;
    });

    interface UserMessageData {
      role: "user";
      content: string;
      created_at: Date;
      files?: Partial<FileAttachment>[];
    }

    const userMessageData: UserMessageData = {
      role: "user",
      content: message || "",
      created_at: new Date(),
    };

    if (fileMetadata && fileMetadata.length > 0) {
      userMessageData.files = fileMetadata;
    }

    const userMessageRef = await conversationRef
      .collection(COLLECTIONS.MESSAGES)
      .add(userMessageData);

    // Get all messages for context
    const messagesSnapshot = await conversationRef
      .collection(COLLECTIONS.MESSAGES)
      .orderBy("created_at", "asc")
      .get();

    const messages: AIMessage[] = messagesSnapshot.docs.map((doc) => {
      let content = doc.data().content;

      // Strip image markdown from content to prevent token overflow
      // (for old messages that still have image data stored)
      // Images should NEVER be included in conversation history for ReAct loop
      if (typeof content === 'string') {
        content = content.replace(/!\[Generated Image\]\(data:image\/[^;]+;base64,[A-Za-z0-9+/=\s]+?\)/g, '[Image was generated here]');
      }

      return {
        role: doc.data().role as "user" | "assistant",
        content,
      };
    });

    // Check for slash commands (/save or /whim)
    const trimmedMessage = message.trim().toLowerCase();
    if (trimmedMessage === '/save' || trimmedMessage === '/whim') {
      console.log('[Chat API] Slash command detected, converting conversation to whim');

      // Exclude the /save command itself from the whim
      // Filter out the last message (which is the /save or /whim command)
      const messagesWithoutCommand = messages.slice(0, -1);

      // Convert conversation to whim (using new JSON blocks format)
      const { title, blocks } = await convertConversationToWhimBlocks(messagesWithoutCommand);

      // Save whim to database (only blocks, no content field for new whims)
      const now = Timestamp.now();
      const whimData: Omit<Whim, 'id'> = {
        userId: session.user.id,
        title,
        blocks,
        conversationId,
        createdAt: now,
        updatedAt: now,
      };

      const whimRef = await db.collection('whims').add(whimData);
      const whimId = whimRef.id;

      console.log('[Chat API] Whim created:', whimId);

      // Return success message as a stream
      const encoder = new TextEncoder();
      const stream = new ReadableStream({
        start(controller) {
          const successMessage = `✅ Whim saved successfully!\n\n**${title}**\n\nYou can view and edit this whim in the [Whims page](/whim).`;
          controller.enqueue(encoder.encode(`[CONTENT]${JSON.stringify(successMessage)}\n`));
          controller.close();
        },
      });

      // Save AI response to conversation
      await conversationRef.collection(COLLECTIONS.MESSAGES).add({
        role: "assistant",
        content: `✅ Whim saved successfully!\n\n**${title}**\n\nYou can view and edit this whim in the Whims page.`,
        created_at: new Date(),
      });

      // Update conversation timestamp
      await conversationRef.update({
        updated_at: new Date(),
      });

      // Cleanup
      removeEmitter(requestId);
      progressUnsubscribe();

      return new Response(stream, {
        headers: {
          "Content-Type": "text/event-stream",
          "Cache-Control": "no-cache",
          Connection: "keep-alive",
          "X-Request-Id": requestId,
        },
      });
    }

    // Check if this is the first message and update title
    if (messages.length === 1 && !isWhimAssistant) {
      const title = await generateConversationTitle(message);
      await conversationRef.update({
        title: title,
        updated_at: new Date(),
      });
    }

    // WHIM ASSISTANT MODE: Use agentic mode with whim context
    if (isWhimAssistant && isAgenticModeEnabled()) {
      console.log('[Chat API] Using whim assistant mode with agentic capabilities');

      // Build whim context to prepend to the message
      let contextPrefix = '';
      if (whimContext.selectedText) {
        contextPrefix = `[Context: I'm working on a document. Here's the selected text I want help with:]\n\n"${whimContext.selectedText}"\n\n[My question:] `;
      } else {
        contextPrefix = `[Context: I'm working on a document with the following content:]\n\n"${whimContext.fullContent}"\n\n[My question:] `;
      }

      // Combine context with user's message
      const messageWithContext = contextPrefix + message;

      // Create stream for agentic response
      let fullResponse = "";

      const stream = new ReadableStream({
        async start(controller) {
          try {
            streamController = controller;

            // Send initial progress event
            const initialProgressEvent: ProgressEvent = {
              step: ProgressStep.ANALYZING_PROMPT,
              status: 'started',
              message: 'Analyzing your question...',
              timestamp: Date.now(),
            };
            controller.enqueue(encoder.encode(`[PROGRESS]${JSON.stringify(initialProgressEvent)}\n`));

            // Create agent (same as regular chat)
            const agent = createAgent({
              userId: session.user.id,
              conversationId,
              style: 'balanced',
              modelTier: conversationData?.model_tier || 'main',
            });

            // Subscribe to agent events
            agent.onEvent((event: AgentEvent) => {
              let progressStep: ProgressStep;
              let progressMessage: string = typeof event.content === 'string' ? event.content : '';

              switch (event.type) {
                case 'reasoning':
                  progressStep = ProgressStep.ANALYZING_PROMPT;
                  progressMessage = 'Reasoning about the question...';
                  break;
                case 'tool_call':
                  if (event.toolName === 'web_search') {
                    progressStep = ProgressStep.SEARCHING_WEB;
                  } else if (event.toolName === 'memory_retrieve' || event.toolName === 'memory_save') {
                    progressStep = ProgressStep.RETRIEVING_MEMORY;
                  } else {
                    progressStep = ProgressStep.BUILDING_CONTEXT;
                  }
                  progressMessage = `Using tool: ${event.toolName}`;
                  break;
                case 'observation':
                  progressStep = ProgressStep.BUILDING_CONTEXT;
                  progressMessage = 'Processing results...';
                  break;
                case 'tool_results':
                  progressStep = ProgressStep.BUILDING_CONTEXT;
                  progressMessage = 'Tool results received';
                  break;
                case 'response':
                  progressStep = ProgressStep.GENERATING_RESPONSE;
                  progressMessage = 'Generating response...';
                  break;
                case 'error':
                  progressStep = ProgressStep.GENERATING_RESPONSE;
                  progressMessage = `Error: ${event.content}`;
                  break;
                default:
                  return;
              }

              const progressEvent: ProgressEvent = {
                step: progressStep,
                status: event.type === 'error' ? 'error' : 'started',
                message: progressMessage,
                timestamp: Date.now(),
              };

              controller.enqueue(encoder.encode(`[PROGRESS]${JSON.stringify(progressEvent)}\n`));
            });

            // Run the agent with message that includes whim context
            // Limit history to last 5 messages to prevent token overflow
            // (each message includes whim context which can be large)
            const historyLimit = 5;
            const limitedHistory = messages.slice(0, -1).slice(-historyLimit);

            const result = await agent.run({
              message: messageWithContext,
              conversationHistory: limitedHistory,
              files,
            });

            fullResponse = result.response;

            // Stream the final response
            const contentLine = `[CONTENT]${JSON.stringify(fullResponse)}\n`;
            controller.enqueue(encoder.encode(contentLine));

            // Send completion event
            const completionEvent: ProgressEvent = {
              step: ProgressStep.GENERATING_RESPONSE,
              status: 'completed',
              message: 'Response complete',
              timestamp: Date.now(),
            };
            controller.enqueue(encoder.encode(`[PROGRESS]${JSON.stringify(completionEvent)}\n`));

            // Unsubscribe and cleanup
            progressUnsubscribe();

            // Save assistant message (save original message in DB, not the context-enhanced version)
            const contentToSave = fullResponse.replace(
              /data:image\/[^;]+;base64,[A-Za-z0-9+/=]+/g,
              'data:image/png;base64,[image-data-removed-due-to-size]'
            );

            await conversationRef.collection(COLLECTIONS.MESSAGES).add({
              role: "assistant",
              content: contentToSave,
              created_at: new Date(),
            });

            await conversationRef.update({
              updated_at: new Date(),
            });

            console.log(`[Chat API] Whim assistant agentic request completed: ${requestId}, iterations: ${result.iterations}, tools: ${result.toolsUsed.join(', ')}`);

            controller.close();
          } catch (error) {
            console.error("Whim assistant streaming error:", error);
            controller.error(error);
          }
        },
      });

      return new Response(stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Transfer-Encoding": "chunked",
          "X-Request-Id": requestId,
        },
      });
    }

    // Create stream immediately to send real-time progress
    let fullResponse = "";

    const stream = new ReadableStream({
      async start(controller) {
        try {
          // Set the stream controller so progress events can be sent in real-time
          streamController = controller;

          // AGENTIC MODE: Use ReAct agent with tools
          if (isAgenticModeEnabled()) {
            console.log('[Chat API] Using agentic mode');

            // Send initial progress event immediately so UI shows feedback
            const initialProgressEvent: ProgressEvent = {
              step: ProgressStep.ANALYZING_PROMPT,
              status: 'started',
              message: 'Analyzing your question...',
              timestamp: Date.now(),
            };
            const initialProgressLine = `[PROGRESS]${JSON.stringify(initialProgressEvent)}\n`;
            controller.enqueue(encoder.encode(initialProgressLine));

            // Create agent with user context
            const agent = createAgent({
              userId: session.user.id,
              conversationId,
              style: 'balanced',
              modelTier: conversationData?.model_tier || 'main',
            });

            // Subscribe to agent events and stream them as progress
            agent.onEvent((event: AgentEvent) => {
              // Map agent events to progress events
              let progressStep: ProgressStep;
              let progressMessage: string = typeof event.content === 'string' ? event.content : '';

              switch (event.type) {
                case 'reasoning':
                  progressStep = ProgressStep.ANALYZING_PROMPT;
                  progressMessage = 'Reasoning about the question...';
                  break;
                case 'tool_call':
                  if (event.toolName === 'web_search') {
                    progressStep = ProgressStep.SEARCHING_WEB;
                  } else if (event.toolName === 'memory_retrieve' || event.toolName === 'memory_save') {
                    progressStep = ProgressStep.RETRIEVING_MEMORY;
                  } else {
                    progressStep = ProgressStep.BUILDING_CONTEXT;
                  }
                  progressMessage = `Using tool: ${event.toolName}`;
                  break;
                case 'observation':
                  progressStep = ProgressStep.BUILDING_CONTEXT;
                  progressMessage = 'Processing results...';
                  break;
                case 'tool_results':
                  progressStep = ProgressStep.BUILDING_CONTEXT;
                  progressMessage = 'Tool results received';
                  break;
                case 'response':
                  progressStep = ProgressStep.GENERATING_RESPONSE;
                  progressMessage = 'Generating response...';
                  break;
                case 'error':
                  progressStep = ProgressStep.GENERATING_RESPONSE;
                  progressMessage = `Error: ${event.content}`;
                  break;
                default:
                  return;
              }

              const progressEvent: ProgressEvent = {
                step: progressStep,
                status: event.type === 'error' ? 'error' : 'started',
                message: progressMessage,
                timestamp: Date.now(),
              };

              const progressLine = `[PROGRESS]${JSON.stringify(progressEvent)}\n`;
              controller.enqueue(encoder.encode(progressLine));
            });

            // Run the agent
            // Limit history to last 10 messages to prevent token overflow
            const historyLimit = 10;
            const limitedHistory = messages.slice(-historyLimit);

            const result = await agent.run({
              message,
              conversationHistory: limitedHistory,
              files,
            });

            fullResponse = result.response;

            // Stream the final response
            const contentLine = `[CONTENT]${JSON.stringify(fullResponse)}\n`;
            controller.enqueue(encoder.encode(contentLine));

            // Send completion event
            const completionEvent: ProgressEvent = {
              step: ProgressStep.GENERATING_RESPONSE,
              status: 'completed',
              message: 'Response complete',
              timestamp: Date.now(),
            };
            const completionLine = `[PROGRESS]${JSON.stringify(completionEvent)}\n`;
            controller.enqueue(encoder.encode(completionLine));

            // Unsubscribe and cleanup
            progressUnsubscribe();

            // Save assistant message WITHOUT image data (Firestore size limits)
            // Strip image markdown before saving
            const contentWithoutImage = fullResponse.replace(/!\[Generated Image\]\(data:image\/[^;]+;base64,[A-Za-z0-9+/=\s]+?\)/g, '[Image generated - not persisted to reduce storage]');

            const messageData: any = {
              role: "assistant",
              content: contentWithoutImage,
              created_at: new Date(),
            };

            // Log if we stripped an image
            if (fullResponse !== contentWithoutImage) {
              console.log('[Chat API] Image markdown stripped from message before Firestore save (size limit)');
            }

            await conversationRef.collection(COLLECTIONS.MESSAGES).add(messageData);

            await conversationRef.update({
              updated_at: new Date(),
            });

            console.log(`[Chat API] Agentic request completed: ${requestId}, iterations: ${result.iterations}, tools: ${result.toolsUsed.join(', ')}`);

            controller.close();
            return;
          }

          // Get active prompt configuration
          const promptConfig = await getActivePrompt();

          // Process prompt variables
          const processedPrompt = processPromptVariables(promptConfig.systemPrompt, {
            userName: session.user.name || "User",
            currentDate: new Date().toLocaleDateString(),
            currentTime: new Date().toLocaleTimeString(),
          });

          // NEW: Intelligent Analysis (if enabled)
          let finalPrompt = processedPrompt;
          let selectedModelName: string | undefined;
          let webSearchResults: SearchResult[] | undefined;
          let extractedFacts: Partial<MemoryFact>[] | undefined;
          let analysis: PromptAnalysisResult | undefined;

          if (isIntelligentAnalysisEnabled()) {
            console.log('[Chat API] Using intelligent analysis');

            // Emit progress: Analyzing prompt
            progressEmitter.emit({
              step: ProgressStep.ANALYZING_PROMPT,
              status: 'started',
              message: 'Analyzing your question...',
              timestamp: Date.now(),
            });

            // Step 1: Analyze user input
            const t1 = Date.now();
            analysis = await promptAnalyzer.analyze({
              message,
              files,
              conversationHistory: messages.slice(-5), // Last 5 messages for context
              userSettings: {
                webSearchEnabled: isWebSearchEnabled(),
                languagePreference: undefined, // Auto-detect
              },
            });
            const t2 = Date.now();
            console.log(`[Performance] PromptAnalysis took ${t2 - t1}ms`);

            // Emit progress: Analysis complete
            progressEmitter.emit({
              step: ProgressStep.ANALYZING_PROMPT,
              status: 'completed',
              message: 'Analysis complete',
              timestamp: Date.now(),
            });

            console.log('[Chat API] Analysis result:', {
              intent: analysis.intent,
              confidence: analysis.confidence,
              actions: Object.keys(analysis.actions).filter(
                (k) => (analysis!.actions as any)[k].needed
              ),
            });

            // Step 1.5: Enhance image generation prompt if needed
            if (analysis.actions.image_generation.needed && analysis.actions.image_generation.description) {
              console.log('[Chat API] Image generation detected - enhancing prompt');

              progressEmitter.emit({
                step: ProgressStep.ANALYZING_PROMPT,
                status: 'started',
                message: 'Enhancing image prompt...',
                timestamp: Date.now(),
              });

              const enhancementResult = await promptEnhancer.enhance(
                analysis.actions.image_generation.description
              );

              // Update analysis with enhanced prompt
              analysis.actions.image_generation.description = enhancementResult.enhancedPrompt;

              console.log('[Chat API] Prompt enhancement:', {
                original: enhancementResult.originalPrompt,
                enhanced: enhancementResult.enhancedPrompt,
                enhancements: enhancementResult.enhancements,
              });

              progressEmitter.emit({
                step: ProgressStep.ANALYZING_PROMPT,
                status: 'completed',
                message: 'Image prompt enhanced',
                timestamp: Date.now(),
              });
            }

            // Emit progress for context preparation steps
            const needsWebSearch = analysis.actions.web_search?.needed;
            const needsMemory = analysis.actions.memory_retrieval?.needed;

            if (needsWebSearch) {
              progressEmitter.emit({
                step: ProgressStep.SEARCHING_WEB,
                status: 'started',
                message: `Searching for: ${analysis.actions.web_search.query}`,
                timestamp: Date.now(),
              });
            }

            if (needsMemory) {
              progressEmitter.emit({
                step: ProgressStep.RETRIEVING_MEMORY,
                status: 'started',
                message: 'Retrieving relevant memories...',
                timestamp: Date.now(),
              });
            }

            // Step 2: Orchestrate context preparation
            const t3 = Date.now();
            const contextResult = await contextOrchestrator.prepare(
              analysis,
              session.user.id,
              conversationId,
              progressEmitter  // Pass progress emitter for content fetching/extraction tracking
            );
            const t4 = Date.now();
            console.log(`[Performance] Context preparation took ${t4 - t3}ms`);

            // Emit progress: Context preparation complete
            if (needsWebSearch) {
              const searchResults = contextResult.webSearchResults || [];
              progressEmitter.emit({
                step: ProgressStep.SEARCHING_WEB,
                status: 'completed',
                message: `Found ${searchResults.length} results`,
                timestamp: Date.now(),
              });
            }

            if (needsMemory) {
              progressEmitter.emit({
                step: ProgressStep.RETRIEVING_MEMORY,
                status: 'completed',
                message: `Retrieved ${contextResult.memoriesRetrieved?.length || 0} memories`,
                timestamp: Date.now(),
              });
            }

            // Emit progress: Building context
            progressEmitter.emit({
              step: ProgressStep.BUILDING_CONTEXT,
              status: 'started',
              message: 'Building context for AI...',
              timestamp: Date.now(),
            });

            // Step 3: Build final prompt with context
            if (contextResult.context) {
              finalPrompt = `${processedPrompt}\n\n${contextResult.context}`;
            }

            // Use selected model from analysis
            selectedModelName = contextResult.modelName;
            webSearchResults = contextResult.webSearchResults;

            // Emit progress: Context built
            progressEmitter.emit({
              step: ProgressStep.BUILDING_CONTEXT,
              status: 'completed',
              message: 'Context ready',
              timestamp: Date.now(),
            });

            // Save extracted facts from analysis
            extractedFacts = analysis.actions.memory_extraction.facts;

            if (contextResult.rateLimitError) {
              console.warn('[Chat API] Rate limit error:', contextResult.rateLimitError);
            }
          } else {
            // OLD: Load user's memory and append to system prompt (keyword-based)
            console.log('[Chat API] Using keyword-based system');

            progressEmitter.emit({
              step: ProgressStep.RETRIEVING_MEMORY,
              status: 'started',
              message: 'Loading context...',
              timestamp: Date.now(),
            });

            const memoryContext = await loadMemoryForChat(session.user.id);
            finalPrompt = memoryContext
              ? `${processedPrompt}\n\n${memoryContext}`
              : processedPrompt;

            progressEmitter.emit({
              step: ProgressStep.RETRIEVING_MEMORY,
              status: 'completed',
              message: 'Context loaded',
              timestamp: Date.now(),
            });
          }

          // Create AI provider instance
          const provider = ProviderFactory.createDefaultProvider(selectedModelName);

          // Emit progress: Starting to generate response
          progressEmitter.emit({
            step: ProgressStep.GENERATING_RESPONSE,
            status: 'started',
            message: 'Generating response...',
            timestamp: Date.now(),
          });

          // Stream AI response content
          for await (const chunk of provider.streamResponse(
            messages,
            finalPrompt,
            promptConfig.temperature,
            files // Pass files with full base64 data to AI
          )) {
            fullResponse += chunk;
            // Send content with [CONTENT] prefix, JSON-encoded to preserve newlines
            const contentLine = `[CONTENT]${JSON.stringify(chunk)}\n`;
            controller.enqueue(encoder.encode(contentLine));
          }

          // Add source citations if we have web search results
          if (webSearchResults && webSearchResults.length > 0) {
            const citations = contextOrchestrator.formatSourceCitations(webSearchResults);
            if (citations) {
              fullResponse += citations;
              const citationLine = `[CONTENT]${JSON.stringify(citations)}\n`;
              controller.enqueue(encoder.encode(citationLine));
            }
          }

          // Send completion progress event directly to ensure it's received
          const completionEvent = {
            step: ProgressStep.GENERATING_RESPONSE,
            status: 'completed' as const,
            message: 'Response complete',
            timestamp: Date.now(),
          };
          const completionLine = `[PROGRESS]${JSON.stringify(completionEvent)}\n`;
          controller.enqueue(encoder.encode(completionLine));
          console.log('[Server] Sent completion event:', completionEvent.step, completionEvent.status);

          // Unsubscribe from progress updates
          progressUnsubscribe();

          // Save assistant message
          // Strip base64 image data to avoid Firestore size limits (1MB max)
          // Images will display during session but won't persist after reload
          const contentToSave = fullResponse.replace(
            /data:image\/[^;]+;base64,[A-Za-z0-9+/=]+/g,
            'data:image/png;base64,[image-data-removed-due-to-size]'
          );

          await conversationRef.collection(COLLECTIONS.MESSAGES).add({
            role: "assistant",
            content: contentToSave,
            created_at: new Date(),
          });

          // Update conversation timestamp
          await conversationRef.update({
            updated_at: new Date(),
          });

          // Post-processing: Save extracted memories or use keyword system
          if (isIntelligentAnalysisEnabled()) {
            // NEW: Save extracted facts from PromptAnalysis
            if (extractedFacts && extractedFacts.length > 0) {
              console.log(`[Chat API] Saving ${extractedFacts.length} extracted facts`);

              // Transform raw facts from PromptAnalyzer into complete MemoryFact objects
              const completedFacts = extractedFacts.map((rawFact: any) => ({
                id: generateMemoryId(),
                content: rawFact.content,
                category: rawFact.category,
                tier: rawFact.tier,
                confidence: rawFact.confidence,
                created_at: new Date(),
                last_used_at: new Date(),
                use_count: 0,
                expires_at: calculateExpiry(rawFact.tier),
                extracted_from: conversationId,
                auto_extracted: true,
                keywords: rawFact.keywords || [],
                source: rawFact.source || 'AI analysis',
              }));


              addMemoryFacts(session.user.id, completedFacts, analysis?.language)
                .then(() => cleanupUserMemory(session.user.id))
                .catch((err) => console.error("Memory save error:", err));
            } else if (analysis?.language) {
              // No facts extracted but language preference detected
              addMemoryFacts(session.user.id, [], analysis.language)
                .catch((err) => console.error("Language preference save error:", err));
            }
          } else {
            // OLD: Check for keyword triggers in user message
            const keywordResults = keywordSystem.check(message);
            const hasMemoryTrigger = keywordResults.some(
              (r) => r.matched && r.type === KeywordTriggerType.MEMORY_GENERAL
            );

            // Execute keyword-triggered actions in background (don't await)
            if (hasMemoryTrigger) {
              keywordSystem
                .execute(KeywordTriggerType.MEMORY_GENERAL, {
                  conversationId,
                  userId: session.user.id,
                  message,
                })
                .then(() => cleanupUserMemory(session.user.id))
                .catch((err) => console.error("Keyword action error:", err));
            }
          }

          console.log(`[Chat API] Completed request: ${requestId}`);

          controller.close();
        } catch (error) {
          console.error("Streaming error:", error);
          controller.error(error);
        }
      },
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "text/plain; charset=utf-8",
        "Transfer-Encoding": "chunked",
        "X-Request-Id": requestId, // Include requestId for tracking
      },
    });
  } catch (error) {
    console.error("Chat API error:", error);
    return new Response("Internal server error", { status: 500 });
  }
}
