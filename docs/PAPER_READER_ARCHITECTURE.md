# Paper Reader Architecture

This document details the implementation architecture for the Paper Reader feature.

## Overview

Paper Reader accepts academic papers in multiple formats, extracts content, analyzes with AI, and outputs structured analysis that can be saved as a Whim.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INPUT LAYER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Academic URL   â”‚  Direct PDF URL â”‚  File Upload                â”‚
â”‚  (arXiv, ACL)   â”‚  (any .pdf)     â”‚  (drag & drop)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                       â”‚
         â–¼                 â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      URL RESOLVER                                â”‚
â”‚  - Detect input type                                             â”‚
â”‚  - Extract PDF URL from academic pages                           â”‚
â”‚  - Validate URLs                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PDF FETCHER                                 â”‚
â”‚  - Fetch PDF bytes (for URLs)                                    â”‚
â”‚  - Handle file uploads                                           â”‚
â”‚  - Validate PDF format                                           â”‚
â”‚  - Cache fetched PDFs                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PDF PARSER                                  â”‚
â”‚  - Extract text content                                          â”‚
â”‚  - Extract metadata (title, authors, abstract)                   â”‚
â”‚  - Handle multi-page documents                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AI ANALYZER                                 â”‚
â”‚  - Send full text to Gemini (2M token context)                   â”‚
â”‚  - Generate structured analysis                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OUTPUT FORMATTER                            â”‚
â”‚  - Structure analysis results                                    â”‚
â”‚  - Convert to Whim-compatible format                             â”‚
â”‚  - Display in UI                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Input Layer

### 1.1 Academic URL Parser

Supported platforms and their URL patterns:

| Platform | URL Pattern | PDF Location |
|----------|-------------|--------------|
| arXiv | `arxiv.org/abs/XXXX.XXXXX` | `arxiv.org/pdf/XXXX.XXXXX.pdf` |
| arXiv (old) | `arxiv.org/abs/category/XXXXXXX` | `arxiv.org/pdf/category/XXXXXXX.pdf` |
| ACL Anthology | `aclanthology.org/XXXX.XXXX/` | `aclanthology.org/XXXX.XXXX.pdf` |
| OpenReview | `openreview.net/forum?id=XXX` | `openreview.net/pdf?id=XXX` |
| Semantic Scholar | `semanticscholar.org/paper/XXX` | API â†’ PDF URL |
| Direct PDF | `*.pdf` | URL itself |

```typescript
// src/lib/paper-reader/url-resolver.ts

interface ResolvedPaper {
  type: 'arxiv' | 'acl' | 'openreview' | 'semantic-scholar' | 'direct-pdf' | 'upload';
  pdfUrl?: string;
  metadata?: {
    title?: string;
    authors?: string[];
    abstract?: string;
    publishedDate?: string;
    venue?: string;
  };
}

function resolveUrl(input: string): Promise<ResolvedPaper>;
```

### 1.2 Direct PDF URL

Simple case - validate URL ends with `.pdf` or has `Content-Type: application/pdf`.

### 1.3 File Upload

```typescript
// Max file size: 20MB (reasonable for academic papers)
const MAX_FILE_SIZE = 20 * 1024 * 1024;

// Accepted MIME types
const ACCEPTED_TYPES = ['application/pdf'];
```

**Security Considerations**:
- Validate MIME type on both client and server
- Check PDF magic bytes (`%PDF-` header)
- Scan for malformed PDFs
- Don't store uploaded files permanently (process in memory)

---

## 2. PDF Fetcher

### 2.1 Fetching Strategy

```typescript
// src/lib/paper-reader/pdf-fetcher.ts

interface FetchResult {
  buffer: ArrayBuffer;
  contentType: string;
  contentLength: number;
  source: 'cache' | 'fetch' | 'upload';
}

async function fetchPdf(url: string): Promise<FetchResult>;
async function handleUpload(file: File): Promise<FetchResult>;
```

### 2.2 Caching Strategy

**Question**: Where to cache?

| Option | Pros | Cons |
|--------|------|------|
| In-memory (LRU) | Fast, simple | Lost on restart, memory pressure |
| Firestore | Persistent, queryable | 1MB doc limit, expensive for PDFs |
| Cloud Storage | Designed for files | Additional service, cost |
| No caching | Simplest | Repeated downloads |

**Recommendation**: Start with no caching. Add in-memory LRU cache later if needed.

### 2.3 Rate Limiting

Protect against abuse and respect upstream services:

```typescript
// Per-user limits
const RATE_LIMITS = {
  fetchesPerHour: 10,
  fetchesPerDay: 50,
  maxConcurrent: 2,
};
```

---

## 3. PDF Parser

### 3.1 Library Options

| Library | Pros | Cons |
|---------|------|------|
| `pdf-parse` | Simple API, popular | Text only, no layout |
| `pdfjs-dist` | Mozilla's library, accurate | Heavier, complex API |
| `pdf2json` | Preserves some layout | Less maintained |
| `unpdf` | Modern, good text extraction | Newer, less tested |

**Recommendation**: Start with `pdf-parse` for simplicity. Evaluate `pdfjs-dist` if text extraction quality is insufficient.

### 3.2 Text Extraction

```typescript
// src/lib/paper-reader/pdf-parser.ts

interface ParsedPaper {
  text: string;              // Full extracted text
  pageCount: number;
  metadata: {
    title?: string;          // From PDF metadata
    author?: string;
    creationDate?: string;
  };
  sections?: Section[];      // If we can detect structure
}

interface Section {
  heading: string;
  content: string;
  page: number;
}

async function parsePdf(buffer: ArrayBuffer): Promise<ParsedPaper>;
```

### 3.3 Full-Text Processing

The READER tier uses Gemini 1.5 Pro with a **2M token context window**. Since academic papers are typically 10-50 pages (~10,000-50,000 tokens), we can always send the full text in a single pass without chunking.

This simplifies the architecture significantly - no need for multi-pass analysis or smart sampling strategies.

---

## 4. AI Analyzer

### 4.1 Analysis Prompt

```typescript
const ANALYSIS_PROMPT = `
Analyze this academic paper and provide a structured analysis:

## Paper Content
{paper_text}

## Required Analysis

Please provide:

### Summary
2-3 sentences summarizing the paper's main contribution.

### Problem Statement
What problem does this paper address? Why is it important?

### Key Contributions
- Bullet point list of main contributions

### Methodology
Describe the technical approach, methods, or algorithms used.

### Results
Key findings, metrics, and experimental results.

### Limitations
What are the acknowledged or apparent limitations?

### Future Work
What directions for future research are suggested?

### Key Takeaways
3-5 bullet points: What should a reader remember from this paper?
`;
```

### 4.2 Model Selection

**New Model Tier: READER**

Paper Reader introduces a new model tier optimized for long-context processing:

| Tier | Model | Context Window | Use Case |
|------|-------|----------------|----------|
| `ModelTier.READER` | Gemini 1.5 Pro | 2M tokens | Paper analysis (long documents) |
| `ModelTier.MAIN` | Gemini 2.5 Flash | 1M tokens | Chat (existing) |
| `ModelTier.LITE` | Gemini 2.5 Flash Lite | 1M tokens | Quick analysis (existing) |

**Why Gemini 1.5 Pro for READER?**
- Optimized for long-context understanding
- 2M token context window (handles any paper)
- Better at maintaining coherence across long documents
- Specifically tuned for document comprehension tasks

```typescript
// Update to src/config/models.ts
export enum ModelTier {
  MAIN = 'main',
  IMAGE = 'image',
  LITE = 'lite',
  READER = 'reader',  // NEW
}

export const MODEL_CONFIG: Record<ModelTier, ModelConfig> = {
  // ... existing tiers
  [ModelTier.READER]: {
    modelId: 'gemini-1.5-pro',
    description: 'Long-context reader optimized for documents',
    contextWindow: 2_000_000,
  },
};
```

### 4.3 Structured Output

Use Gemini's structured output mode for consistent formatting:

```typescript
const analysisSchema = {
  type: "object",
  properties: {
    summary: { type: "string" },
    problemStatement: { type: "string" },
    keyContributions: {
      type: "array",
      items: { type: "string" }
    },
    methodology: { type: "string" },
    results: { type: "string" },
    limitations: { type: "string" },
    futureWork: { type: "string" },
    keyTakeaways: {
      type: "array",
      items: { type: "string" }
    },
  },
  required: ["summary", "keyContributions", "keyTakeaways"]
};
```

---

## 5. Output & Whim Integration

### 5.1 Display Format

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“„ [Paper Title]                                                â”‚
â”‚  Authors: [Author List]                                          â”‚
â”‚  Published: [Date] | Venue: [Conference/Journal]                 â”‚
â”‚  Source: [arXiv:XXXX.XXXXX]                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ## Summary                                                      â”‚
â”‚  [2-3 sentence summary]                                          â”‚
â”‚                                                                  â”‚
â”‚  ## Problem Statement                                            â”‚
â”‚  [Problem description]                                           â”‚
â”‚                                                                  â”‚
â”‚  ## Key Contributions                                            â”‚
â”‚  â€¢ [Contribution 1]                                              â”‚
â”‚  â€¢ [Contribution 2]                                              â”‚
â”‚  â€¢ [Contribution 3]                                              â”‚
â”‚                                                                  â”‚
â”‚  ## Methodology                                                  â”‚
â”‚  [Technical approach]                                            â”‚
â”‚                                                                  â”‚
â”‚  ## Results                                                      â”‚
â”‚  [Key findings]                                                  â”‚
â”‚                                                                  â”‚
â”‚  ## Limitations                                                  â”‚
â”‚  [Acknowledged weaknesses]                                       â”‚
â”‚                                                                  â”‚
â”‚  ## Future Work                                                  â”‚
â”‚  [Suggested directions]                                          â”‚
â”‚                                                                  â”‚
â”‚  ## Key Takeaways                                                â”‚
â”‚  â€¢ [Takeaway 1]                                                  â”‚
â”‚  â€¢ [Takeaway 2]                                                  â”‚
â”‚                                                                  â”‚
â”‚  ## My Notes                                                     â”‚
â”‚  [Editable area for user annotations]                            â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Save as Whim]  [Copy Markdown]  [Analyze Another]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Concrete Example: "Attention Is All You Need"

Here's what an actual analysis looks like:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“„ Attention Is All You Need                                   â”‚
â”‚  Authors: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, et al.   â”‚
â”‚  Published: 2017 | Venue: NeurIPS                               â”‚
â”‚  Source: arXiv:1706.03762                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  ## Summary                                                      â”‚
â”‚  This paper introduces the Transformer, a novel architecture    â”‚
â”‚  that relies entirely on self-attention mechanisms, dispensing  â”‚
â”‚  with recurrence and convolutions. It achieves state-of-the-art â”‚
â”‚  results on machine translation while being more parallelizable.â”‚
â”‚                                                                  â”‚
â”‚  ## Problem Statement                                            â”‚
â”‚  Sequential models like RNNs and LSTMs have inherent            â”‚
â”‚  limitations: they process tokens sequentially, preventing      â”‚
â”‚  parallelization and struggling with long-range dependencies.   â”‚
â”‚                                                                  â”‚
â”‚  ## Key Contributions                                            â”‚
â”‚  â€¢ Introduced the Transformer architecture based solely on      â”‚
â”‚    attention mechanisms                                          â”‚
â”‚  â€¢ Proposed multi-head attention to jointly attend to           â”‚
â”‚    information from different representation subspaces          â”‚
â”‚  â€¢ Achieved 28.4 BLEU on WMT 2014 English-to-German translation â”‚
â”‚  â€¢ Reduced training time significantly through parallelization  â”‚
â”‚                                                                  â”‚
â”‚  ## Methodology                                                  â”‚
â”‚  The Transformer uses an encoder-decoder structure with         â”‚
â”‚  stacked self-attention and point-wise fully connected layers.  â”‚
â”‚  Key components include:                                         â”‚
â”‚  - Scaled dot-product attention: Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
â”‚  - Multi-head attention: 8 parallel attention layers            â”‚
â”‚  - Positional encoding: sine/cosine functions for position info â”‚
â”‚                                                                  â”‚
â”‚  ## Results                                                      â”‚
â”‚  - WMT 2014 EN-DE: 28.4 BLEU (new SOTA, +2.0 over previous)    â”‚
â”‚  - WMT 2014 EN-FR: 41.0 BLEU (new SOTA)                         â”‚
â”‚  - Training time: 3.5 days on 8 P100 GPUs                       â”‚
â”‚  - 10x less training cost than competing models                  â”‚
â”‚                                                                  â”‚
â”‚  ## Limitations                                                  â”‚
â”‚  - Memory grows quadratically with sequence length (O(nÂ²))      â”‚
â”‚  - Positional encodings are fixed, not learned                  â”‚
â”‚  - Evaluated primarily on translation tasks                      â”‚
â”‚                                                                  â”‚
â”‚  ## Future Work                                                  â”‚
â”‚  - Apply to other modalities (images, audio, video)             â”‚
â”‚  - Investigate local attention for very long sequences          â”‚
â”‚  - Explore learned positional representations                    â”‚
â”‚                                                                  â”‚
â”‚  ## Key Takeaways                                                â”‚
â”‚  â€¢ Self-attention can fully replace recurrence for seq2seq      â”‚
â”‚  â€¢ Parallelization dramatically speeds up training              â”‚
â”‚  â€¢ Multi-head attention captures different types of relations   â”‚
â”‚  â€¢ This architecture became the foundation for BERT, GPT, etc.  â”‚
â”‚                                                                  â”‚
â”‚  ## My Notes                                                     â”‚
â”‚  [Empty - user can add their own annotations here]              â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Save as Whim]  [Copy Markdown]  [Analyze Another]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API Response (JSON)**:

```json
{
  "metadata": {
    "title": "Attention Is All You Need",
    "authors": ["Vaswani", "Shazeer", "Parmar", "Uszkoreit", "Jones", "..."],
    "publishedDate": "2017",
    "venue": "NeurIPS",
    "sourceUrl": "https://arxiv.org/abs/1706.03762",
    "arxivId": "1706.03762"
  },
  "analysis": {
    "summary": "This paper introduces the Transformer, a novel architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions. It achieves state-of-the-art results on machine translation while being more parallelizable.",
    "problemStatement": "Sequential models like RNNs and LSTMs have inherent limitations: they process tokens sequentially, preventing parallelization and struggling with long-range dependencies.",
    "keyContributions": [
      "Introduced the Transformer architecture based solely on attention mechanisms",
      "Proposed multi-head attention to jointly attend to information from different representation subspaces",
      "Achieved 28.4 BLEU on WMT 2014 English-to-German translation",
      "Reduced training time significantly through parallelization"
    ],
    "methodology": "The Transformer uses an encoder-decoder structure with stacked self-attention and point-wise fully connected layers. Key components include scaled dot-product attention, multi-head attention (8 parallel layers), and positional encoding using sine/cosine functions.",
    "results": "WMT 2014 EN-DE: 28.4 BLEU (new SOTA, +2.0 over previous). WMT 2014 EN-FR: 41.0 BLEU (new SOTA). Training time: 3.5 days on 8 P100 GPUs. 10x less training cost than competing models.",
    "limitations": "Memory grows quadratically with sequence length (O(nÂ²)). Positional encodings are fixed, not learned. Evaluated primarily on translation tasks.",
    "futureWork": "Apply to other modalities (images, audio, video). Investigate local attention for very long sequences. Explore learned positional representations.",
    "keyTakeaways": [
      "Self-attention can fully replace recurrence for seq2seq",
      "Parallelization dramatically speeds up training",
      "Multi-head attention captures different types of relations",
      "This architecture became the foundation for BERT, GPT, etc."
    ]
  }
}
```

### 5.3 Whim Conversion

When user clicks "Save as Whim", convert to TipTap JSON blocks:

```typescript
// src/lib/paper-reader/whim-converter.ts

function analysisToWhimBlocks(analysis: PaperAnalysis): TipTapBlock[] {
  return [
    { type: 'heading', attrs: { level: 1 }, content: [{ type: 'text', text: analysis.title }] },
    { type: 'paragraph', content: [{ type: 'text', text: `Authors: ${analysis.authors.join(', ')}` }] },
    // ... more blocks for each section
  ];
}
```

### 5.4 Metadata Storage

Store paper metadata for future reference:

```typescript
interface PaperWhimMetadata {
  type: 'paper-analysis';
  sourceUrl?: string;
  arxivId?: string;
  doi?: string;
  authors: string[];
  publishedDate?: string;
  venue?: string;
  analyzedAt: Date;
}
```

---

## 6. API Design

### 6.1 Endpoints

```
POST /api/paper/analyze
  Body: { url: string } | FormData with file
  Response: { jobId: string }

GET /api/paper/progress/:jobId  (SSE)
  Response: Stream of { stage, progress, result?, error? }

GET /api/paper/result/:jobId
  Response: { status, analysis?, error? }
  (For clients that don't support SSE)
```

### 6.2 Processing Flow (Async with Progress Tracking)

Paper analysis uses async processing with real-time progress updates:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      POST /api/paper/analyze      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Server  â”‚
â”‚         â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚         â”‚
â”‚         â”‚      { jobId: "abc123" }          â”‚         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                                              â”‚
     â”‚         SSE: /api/paper/progress/abc123      â”‚
     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚  { stage: "fetching", progress: 10 }         â”‚
     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚  { stage: "parsing", progress: 30 }          â”‚
     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚  { stage: "analyzing", progress: 60 }        â”‚
     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚  { stage: "complete", progress: 100,         â”‚
     â”‚    analysis: { ... } }                       â”‚
     â–¼                                              â–¼
```

**Processing Stages**:

| Stage | Progress | Description |
|-------|----------|-------------|
| `validating` | 0-5% | Validate URL/file |
| `fetching` | 5-20% | Download PDF |
| `parsing` | 20-40% | Extract text |
| `analyzing` | 40-95% | AI analysis |
| `formatting` | 95-100% | Format output |
| `complete` | 100% | Done |
| `error` | - | Failed |

**Implementation**:

```typescript
// Job status stored in memory (or Redis for multi-instance)
interface AnalysisJob {
  id: string;
  userId: string;
  status: 'pending' | 'processing' | 'complete' | 'error';
  stage: string;
  progress: number;
  result?: PaperAnalysis;
  error?: string;
  createdAt: Date;
  updatedAt: Date;
}

// Progress updates via Server-Sent Events (SSE)
// src/app/api/paper/progress/[jobId]/route.ts
export async function GET(req: Request, { params }: { params: { jobId: string } }) {
  const stream = new ReadableStream({
    start(controller) {
      const interval = setInterval(() => {
        const job = getJob(params.jobId);
        controller.enqueue(`data: ${JSON.stringify(job)}\n\n`);
        if (job.status === 'complete' || job.status === 'error') {
          clearInterval(interval);
          controller.close();
        }
      }, 500);
    }
  });

  return new Response(stream, {
    headers: { 'Content-Type': 'text/event-stream' }
  });
}
```

**UI Progress Indicator**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analyzing paper...                                              â”‚
â”‚                                                                  â”‚
â”‚  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 35%                  â”‚
â”‚                                                                  â”‚
â”‚  ğŸ“¥ Fetching PDF âœ“                                               â”‚
â”‚  ğŸ“„ Parsing content âœ“                                            â”‚
â”‚  ğŸ¤– Analyzing with AI...                                         â”‚
â”‚  ğŸ“ Formatting output                                            â”‚
â”‚                                                                  â”‚
â”‚  Estimated time remaining: ~30 seconds                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. File Structure

```
src/lib/paper-reader/
â”œâ”€â”€ index.ts                 # Public exports
â”œâ”€â”€ url-resolver.ts          # URL parsing and PDF URL extraction
â”œâ”€â”€ pdf-fetcher.ts           # PDF downloading and caching
â”œâ”€â”€ pdf-parser.ts            # PDF text extraction
â”œâ”€â”€ analyzer.ts              # AI analysis logic
â”œâ”€â”€ whim-converter.ts        # Convert analysis to Whim blocks
â”œâ”€â”€ types.ts                 # TypeScript interfaces
â””â”€â”€ platforms/               # Platform-specific parsers
    â”œâ”€â”€ arxiv.ts
    â”œâ”€â”€ acl.ts
    â”œâ”€â”€ openreview.ts
    â””â”€â”€ semantic-scholar.ts

src/app/
â”œâ”€â”€ paper/
â”‚   â””â”€â”€ page.tsx             # Paper Reader UI
â””â”€â”€ api/paper/
    â”œâ”€â”€ analyze/route.ts     # Start analysis, returns jobId
    â”œâ”€â”€ progress/[jobId]/route.ts  # SSE progress stream
    â””â”€â”€ result/[jobId]/route.ts    # Get final result (fallback)

src/components/paper-reader/
â”œâ”€â”€ PaperInput.tsx           # URL input + file upload
â”œâ”€â”€ PaperAnalysis.tsx        # Analysis display
â””â”€â”€ PaperActions.tsx         # Save/Copy/Share buttons
```

---

## 8. Error Handling

| Error | User Message | Action |
|-------|--------------|--------|
| Invalid URL | "Please enter a valid paper URL" | Show input error |
| PDF fetch failed | "Couldn't download the paper. Check the URL or try uploading." | Suggest upload |
| PDF parse failed | "Couldn't read this PDF. It may be scanned or protected." | Explain limitation |
| Rate limited | "You've analyzed many papers today. Try again in X hours." | Show limit info |
| Analysis failed | "Analysis failed. Please try again." | Offer retry |
| File too large | "File is too large (max 20MB)" | Show limit |

---

## 9. Security Considerations

1. **URL Validation**: Only allow HTTP(S) URLs
2. **SSRF Prevention**: Block internal/private IP ranges
3. **File Validation**: Check PDF magic bytes, not just extension
4. **Rate Limiting**: Prevent abuse
5. **No Persistent Storage**: Don't store uploaded PDFs
6. **Sanitize Output**: Clean AI output before display

---

## 10. Decisions & Open Questions

### Decided

| Question | Decision | Notes |
|----------|----------|-------|
| Async vs Sync Processing | **Async with SSE progress tracking** | See Section 6.2 |
| Model for Analysis | **Gemini 1.5 Pro (READER tier)** | 2M context, optimized for long docs |

### Open Questions (To Be Discussed Later)

1. **PDF Text Quality**
   - How to handle scanned PDFs (images)?
   - OCR needed? (adds complexity)

2. **Math and Figures**
   - Ignore figures for now?
   - Extract figure captions?
   - How to represent equations in text?

3. **Caching Strategy**
   - Cache parsed text? Cache analysis?
   - Cache key: URL hash? Content hash?

4. **Citation Extraction**
   - Extract and link references?
   - Would require parsing bibliography

5. **Multi-Language Papers**
   - Support non-English papers?
   - Auto-detect and translate?

6. **Batch Processing**
   - Analyze multiple papers?
   - Compare papers?

7. **Integration with Memory**
   - Auto-save paper summaries to memory?
   - Link related papers?

---

## 11. Implementation Phases

### Phase 1: MVP
- [ ] URL input (arXiv only)
- [ ] PDF fetching
- [ ] Basic text extraction
- [ ] AI analysis with structured output
- [ ] Display results
- [ ] Save as Whim

### Phase 2: Enhanced Input
- [ ] Direct PDF URL support
- [ ] File upload
- [ ] More platforms (ACL, OpenReview)
- [ ] Better metadata extraction

### Phase 3: Polish
- [ ] Progress indicator
- [ ] Error handling improvements
- [ ] Rate limiting
- [ ] Caching

### Phase 4: Advanced Features
- [ ] Citation extraction
- [ ] Figure caption extraction
- [ ] Batch processing
- [ ] Memory integration

---

**Created**: December 1, 2025
**Status**: Draft - Awaiting Review
